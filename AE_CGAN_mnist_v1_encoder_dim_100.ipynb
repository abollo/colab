{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AE_CGAN_mnist_v1_encoder_dim-100.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNWlDs0FoDrNoTfRHqkZ8GK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abollo/colab/blob/master/AE_CGAN_mnist_v1_encoder_dim_100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx0viG3fFXgf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e3895591-19ca-4bab-f34b-b5b59663f03f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN2a-B1lChEd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "cefaa183-af01-4984-a20a-3838f43f14e8"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "'''\n",
        "information gan add Autoencoder\n",
        "'''\n",
        "\n",
        "'''\n",
        "mount the google drive \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "'''\n",
        "import tensorflow as tf\n",
        "from numpy.random import randn\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "from numpy.linalg import lstsq\n",
        "from scipy.linalg import orth\n",
        "from scipy.stats import ortho_group\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# classes dictionary\n",
        "label_dict = {0: 'tshirt',\n",
        "\t\t\t 1: 'trouser',\n",
        "\t\t\t 2: 'pullover',\n",
        "\t\t\t 3: 'dress',\n",
        "\t\t\t 4: 'coat',\n",
        "\t\t\t 5: 'sandal',\n",
        "\t\t\t 6: 'shirt',\n",
        "\t\t\t 7: 'sneaker',\n",
        "\t\t\t 8: 'bag',\n",
        "\t\t\t 9: 'boot'}\n",
        " \n",
        "def load_minst_data():\n",
        "\t# load the data\n",
        "\t(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\t# normalize our inputs to be in the range[-1, 1]\n",
        "\tx_train = (x_train.astype(np.float32) - 127.5)/127.5\n",
        "\t# convert x_train with a shape of (60000, 28, 28) to (60000, 784) so we have\n",
        "\t# 784 columns per row\n",
        "\treturn (x_train, y_train, x_test, y_test)\n",
        " \n",
        "x_train, y_train, x_test, y_test = load_minst_data()\n",
        "input_classes =pd.Series(y_train).nunique()\n",
        "print(\"x_train shape: {}\".format(x_train.shape))\n",
        "print(\"y_train.shape:{}\".format(y_train.shape))\n",
        "\n",
        "class ACGAN():\n",
        "\tdef __init__(self, input_rows, input_cols, input_channels, input_classes, latent_dim=100,encoder_dim=100):\n",
        "\t\t# Input shape\n",
        "\t\tself.img_rows = input_rows\n",
        "\t\tself.img_cols = input_cols\n",
        "\t\tself.channels = input_channels\n",
        "\t\tself.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "\t\tself.num_classes = input_classes\n",
        "\t\tself.latent_dim = latent_dim\n",
        "\t\tself.encoder_dim = encoder_dim\n",
        "\n",
        "\t\toptimizer = tf.keras.optimizers.Adam(0.0002, 0.5)\n",
        "\t\tlosses2 = ['binary_crossentropy', 'mean_absolute_error']\n",
        "\t\tlosses3 = [ 'mean_absolute_error']\n",
        "\t\tlosses4 = ['binary_crossentropy']\n",
        "\t\tlosses = ['binary_crossentropy', 'sparse_categorical_crossentropy']\n",
        "\t\tlosses_encoder = ['sparse_categorical_crossentropy','mean_absolute_error']\n",
        "\n",
        "\n",
        "\t\t# Build and compile the discriminator\n",
        "\t\tself.discriminator = self.build_discriminator2()\n",
        "\t\tself.discriminator.compile(loss=losses4,optimizer=optimizer,metrics=['accuracy'])\n",
        "\n",
        "\t\t# Build the generator\n",
        "\t\tself.generator = self.build_generator()\n",
        "\t\t#self.encoder = self.build_encoder()\n",
        "\t\tself.AEencoder = self.build_AEencoder()\n",
        "\t\tself.AEdecoder = self.build_AEdecoder()\n",
        "\t\tself.AEdecoder.summary()\n",
        "\n",
        "\t\t# The generator takes noise and the target label as input\n",
        "\t\t# and generates the corresponding digit of that label\n",
        "\t\tnoise = tf.keras.layers.Input(shape=(self.latent_dim,))\n",
        "\t\timg_label = tf.keras.layers.Input(shape=self.img_shape)\n",
        "\t\tae_labels = self.AEencoder(img_label)\n",
        "\t\timg = self.generator([noise, ae_labels])\n",
        "\n",
        "\t\t# For the combined model we will only train the generator\n",
        "\t\tself.discriminator.trainable = False\n",
        "\t\t\n",
        "\n",
        "\t\t# The discriminator takes generated image as input and determines validity\n",
        "\t\t# and the label of that image\n",
        "\t\tvalid = self.discriminator([img, ae_labels])\n",
        "\n",
        "\t\t# The combined model  (stacked generator and discriminator)\n",
        "\t\t# Trains the generator to fool the discriminator\n",
        "\t\tself.combined = tf.keras.Model([noise, img_label], valid)\n",
        "\t\tself.combined.compile(loss=losses4,optimizer=optimizer)\n",
        "\n",
        "\t\t#encoder_latent = tf.keras.layers.Input(shape=(self.encoder_dim,))\n",
        "\t\timg_de = self.AEdecoder(ae_labels)\n",
        "\t\t\n",
        "\t\tself.discriminator.trainable = False\n",
        "\t\tself.generator.trainable = False\n",
        "\n",
        "\t\tself.combined_AE = tf.keras.Model(img_label, img_de)\n",
        "\t\tself.combined_AE.compile(loss=losses3,optimizer=optimizer)\n",
        "\n",
        "\t\t\n",
        "\n",
        "\t\t#self.combined2 = tf.keras.Model(img_label, disc_labels)\n",
        "\t\t#self.combined2.compile(loss=losses3,optimizer=optimizer)\n",
        "\n",
        "\tdef build_generator(self):\n",
        "\n",
        "\t\tnoise_input = tf.keras.layers.Input(shape=(self.latent_dim,))\n",
        "\t\tlabel_input = tf.keras.layers.Input(shape=(self.encoder_dim,))\n",
        "\t\tlabel=tf.keras.layers.Dense(self.latent_dim*3, activation=\"relu\")(label_input)\n",
        "\t\tlabel=tf.keras.layers.Dense(self.img_shape[0] * self.img_shape[1], activation=\"relu\")(label)\n",
        "\t\tlabel =tf.keras.layers.Reshape((self.img_shape[0], self.img_shape[1], 1))(label)\n",
        "\t\tnoise=tf.keras.layers.Dense(self.latent_dim*3, activation=\"relu\")(noise_input)\n",
        "\t\tnoise=tf.keras.layers.Dense(self.img_shape[0] * self.img_shape[1], activation=\"relu\")(noise)\n",
        "\t\tnoise =tf.keras.layers.Reshape((self.img_shape[0], self.img_shape[1], 1))(noise)\n",
        "\t\t#model_input = tf.keras.layers.multiply([noise, label])\n",
        "\t\tmerge = tf.keras.layers.Concatenate()([noise, label])\n",
        "\t\t#g_x = tf.keras.layers.Dense(128 * 7 * 7, activation=\"relu\")(merge)\n",
        "\t\t#g_x = tf.keras.layers.Reshape((7, 7, 128))(g_x)\n",
        "\t\t#g_x = tf.keras.layers.BatchNormalization(momentum=0.8)(g_x)\n",
        "\t\t#g_x = tf.keras.layers.UpSampling2D()(g_x)\n",
        "\t\tg_x = tf.keras.layers.Conv2D(128, kernel_size=3, padding=\"same\")(merge)\n",
        "\t\tg_x = tf.keras.layers.Activation(\"relu\")(g_x)\n",
        "\t\tg_x = tf.keras.layers.BatchNormalization(momentum=0.8)(g_x)\n",
        "\t\t#g_x = tf.keras.layers.UpSampling2D()(g_x)\n",
        "\t\tg_x = tf.keras.layers.Conv2D(256, kernel_size=3, padding=\"same\")(g_x)\n",
        "\t\tg_x = tf.keras.layers.Activation(\"relu\")(g_x)\n",
        "\t\tg_x = tf.keras.layers.BatchNormalization(momentum=0.8)(g_x)\n",
        "\t\tg_x = tf.keras.layers.Conv2D(128, kernel_size=3, padding=\"same\")(g_x)\n",
        "\t\tg_x = tf.keras.layers.Activation(\"relu\")(g_x)\n",
        "\t\tg_x = tf.keras.layers.BatchNormalization(momentum=0.8)(g_x)\n",
        "\t\tg_x = tf.keras.layers.Conv2D(self.channels, kernel_size=3, padding='same')(g_x)\n",
        "\t\tg_x = tf.keras.layers.Activation(\"tanh\")(g_x)\n",
        "\n",
        "\t\treturn tf.keras.Model(inputs=[noise_input,label_input],outputs=g_x)\n",
        "\n",
        "\tdef build_discriminator(self):\n",
        "\t\timg_input = tf.keras.layers.Input(shape=self.img_shape)\n",
        "\t\tlabel_input = tf.keras.layers.Input(shape=(self.encoder_dim,))\n",
        "\t\tlabel=tf.keras.layers.Dense(128 * 7 * 7, activation=\"relu\")(label_input)\n",
        "\t\tlabel = tf.keras.layers.Dense(self.img_shape[0] * self.img_shape[1], activation=\"relu\")(label_input)\n",
        "\t\tlabel =tf.keras.layers.Reshape((self.img_shape[0], self.img_shape[1], 1))(label)\n",
        "\t\tmerge = tf.keras.layers.Concatenate()([img_input, label])\n",
        "\t\td_x = tf.keras.layers.Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\")(merge)\n",
        "\t\td_x = tf.keras.layers.LeakyReLU(alpha=0.2)(d_x)\n",
        "\t\td_x = tf.keras.layers.Dropout(0.25)(d_x)\n",
        "\t\td_x = tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, padding=\"same\")(d_x)\n",
        "\t\td_x = tf.keras.layers.ZeroPadding2D(padding=((0,1),(0,1)))(d_x)\n",
        "\t\td_x = tf.keras.layers.LeakyReLU(alpha=0.2)(d_x)\n",
        "\t\td_x = tf.keras.layers.Dropout(0.25)(d_x)\n",
        "\t\td_x = tf.keras.layers.BatchNormalization(momentum=0.8)(d_x)\n",
        "\t\td_x = tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, padding=\"same\")(d_x)\n",
        "\t\td_x = tf.keras.layers.LeakyReLU(alpha=0.2)(d_x)\n",
        "\t\td_x = tf.keras.layers.Dropout(0.25)(d_x)\n",
        "\t\td_x = tf.keras.layers.BatchNormalization(momentum=0.8)(d_x)\n",
        "\t\td_x = tf.keras.layers.Conv2D(128, kernel_size=3, strides=1, padding=\"same\")(d_x)\n",
        "\t\td_x = tf.keras.layers.LeakyReLU(alpha=0.2)(d_x)\n",
        "\t\td_x = tf.keras.layers.Dropout(0.25)(d_x)\n",
        "\t\td_x = tf.keras.layers.Flatten()(d_x)\n",
        "\t\td_x = tf.keras.layers.Dense(self.latent_dim*4, activation=\"relu\")(d_x)\n",
        "\t\td_x1 = tf.keras.layers.Dense(self.latent_dim, activation=\"relu\")(d_x)\n",
        "\t\tvalidity = tf.keras.layers.Dense(1, activation=\"sigmoid\")(d_x1)\n",
        "\t\td_x2 = tf.keras.layers.Dense(self.latent_dim*2, activation=\"relu\")(d_x)\n",
        "\t\td_x_output = tf.keras.layers.Dense(self.num_classes+1, activation=\"sigmoid\")(d_x2)\n",
        "\n",
        "\t\treturn tf.keras.Model(inputs=[img_input,label_input],outputs=[validity,d_x_output])\n",
        "\n",
        "\tdef build_discriminator2(self):\n",
        "\t\timg_input = tf.keras.layers.Input(shape=self.img_shape)\n",
        "\t\tlabel_input = tf.keras.layers.Input(shape=(self.encoder_dim,))\n",
        "\t\tlabel=tf.keras.layers.Dense(128 * 7 * 7, activation=\"relu\")(label_input)\n",
        "\t\tlabel = tf.keras.layers.Dense(self.img_shape[0] * self.img_shape[1], activation=\"relu\")(label_input)\n",
        "\t\tlabel =tf.keras.layers.Reshape((self.img_shape[0], self.img_shape[1], 1))(label)\n",
        "\t\tmerge = tf.keras.layers.Concatenate()([img_input, label])\n",
        "\t\td_x = tf.keras.layers.Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\")(merge)\n",
        "\t\td_x = tf.keras.layers.LeakyReLU(alpha=0.2)(d_x)\n",
        "\t\td_x = tf.keras.layers.Dropout(0.25)(d_x)\n",
        "\t\td_x = tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, padding=\"same\")(d_x)\n",
        "\t\td_x = tf.keras.layers.ZeroPadding2D(padding=((0,1),(0,1)))(d_x)\n",
        "\t\td_x = tf.keras.layers.LeakyReLU(alpha=0.2)(d_x)\n",
        "\t\td_x = tf.keras.layers.Dropout(0.25)(d_x)\n",
        "\t\td_x = tf.keras.layers.BatchNormalization(momentum=0.8)(d_x)\n",
        "\t\td_x = tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, padding=\"same\")(d_x)\n",
        "\t\td_x = tf.keras.layers.LeakyReLU(alpha=0.2)(d_x)\n",
        "\t\td_x = tf.keras.layers.Dropout(0.25)(d_x)\n",
        "\t\td_x = tf.keras.layers.BatchNormalization(momentum=0.8)(d_x)\n",
        "\t\td_x = tf.keras.layers.Conv2D(128, kernel_size=3, strides=1, padding=\"same\")(d_x)\n",
        "\t\td_x = tf.keras.layers.LeakyReLU(alpha=0.2)(d_x)\n",
        "\t\td_x = tf.keras.layers.Dropout(0.25)(d_x)\n",
        "\t\td_x = tf.keras.layers.Flatten()(d_x)\n",
        "\t\td_x = tf.keras.layers.Dense(self.latent_dim*4, activation=\"relu\")(d_x)\n",
        "\t\td_x1 = tf.keras.layers.Dense(self.latent_dim, activation=\"relu\")(d_x)\n",
        "\t\tvalidity = tf.keras.layers.Dense(1, activation=\"sigmoid\")(d_x1)\n",
        "\t\t#d_x2 = tf.keras.layers.Dense(self.latent_dim*2, activation=\"relu\")(d_x)\n",
        "\t\t#d_x_output = tf.keras.layers.Dense(self.num_classes+1, activation=\"sigmoid\")(d_x2)\n",
        "\n",
        "\t\treturn tf.keras.Model(inputs=[img_input,label_input],outputs=validity)\n",
        "\n",
        "\tdef build_AEencoder(self):\n",
        "\t\t# Network parameters\n",
        "\t\tkernel_size = 3\n",
        "\t\t# Encoder/Decoder number of CNN layers and filters per layer\n",
        "\t\tlayer_filters = [32, 64, 128]\n",
        "\t\timg_input = tf.keras.layers.Input(shape=self.img_shape)\n",
        "\n",
        "\t\tx = img_input\n",
        "\t\t# Stack of Conv2D blocks\n",
        "\t\t# Notes:\n",
        "\t\t# 1) Use Batch Normalization before ReLU on deep networks\n",
        "\t\t# 2) Use MaxPooling2D as alternative to strides>1\n",
        "\t\t# - faster but not as good as strides>1\n",
        "\t\tfor filters in layer_filters:\n",
        "\t\t\tx = tf.keras.layers.Conv2D(filters=filters,\n",
        "\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\tstrides=2,\n",
        "\t\t\t\tactivation='relu',\n",
        "\t\t\t\tpadding='same')(x)\n",
        "\n",
        "\t\t# Shape info needed to build Decoder Model\n",
        "\t\t#shape = K.int_shape(x)\n",
        "\t\t#(4,4,128)\n",
        "\n",
        "\t\t# Generate the latent vector\n",
        "\t\tx = tf.keras.layers.Flatten()(x)\n",
        "\t\tlatent = tf.keras.layers.Dense(self.encoder_dim, name='encoder_dim_latent_vector')(x)\n",
        "\n",
        "\t\treturn tf.keras.Model(inputs=img_input,outputs=latent)\n",
        "\n",
        "\tdef build_AEdecoder(self):\n",
        "\t\t# Network parameters\n",
        "\t\tkernel_size = 3\n",
        "\t\t# Encoder/Decoder number of CNN layers and filters per layer\n",
        "\t\tlayer_filters = [64, 128]\n",
        "\t\tlabel_input = tf.keras.layers.Input(shape=(self.encoder_dim,))\n",
        "\n",
        "\t\tx = tf.keras.layers.Dense(3*3*128)(label_input)\n",
        "\t\tx = tf.keras.layers.Reshape((3,3, 128))(x)\n",
        "\t\tx = tf.keras.layers.Conv2DTranspose(filters=32,\n",
        "\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\tstrides=2,\n",
        "\t\t\t\tactivation='relu',\n",
        "\t\t\t\tpadding='valid')(x)\n",
        "\t\t# Stack of Transposed Conv2D blocks\n",
        "\t\t# Notes:\n",
        "\t\t# 1) Use Batch Normalization before ReLU on deep networks\n",
        "\t\t# 2) Use UpSampling2D as alternative to strides>1\n",
        "\t\t# - faster but not as good as strides>1\n",
        "\t\tfor filters in layer_filters[::-1]:\n",
        "\t\t\tx = tf.keras.layers.Conv2DTranspose(filters=filters,\n",
        "\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\tstrides=2,\n",
        "\t\t\t\tactivation='relu',\n",
        "\t\t\t\tpadding='same')(x)\n",
        "\n",
        "\t\tx = tf.keras.layers.Conv2DTranspose(filters=1,kernel_size=kernel_size,padding='same')(x)\n",
        "\n",
        "\t\toutputs = tf.keras.layers.Activation('sigmoid', name='decoder_output')(x)\n",
        "\n",
        "\t\treturn tf.keras.Model(inputs=label_input,outputs=outputs)\n",
        "\n",
        "\t\n",
        "\n",
        "\tdef build_encoder2(self):\n",
        "\t\timg_input = tf.keras.layers.Input(shape=self.img_shape)\n",
        "\t\td_x = tf.keras.layers.Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\")(img_input)\n",
        "\t\td_x = tf.keras.layers.LeakyReLU(alpha=0.2)(d_x)\n",
        "\t\td_x = tf.keras.layers.Dropout(0.25)(d_x)\n",
        "\t\td_x = tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, padding=\"same\")(d_x)\n",
        "\t\td_x = tf.keras.layers.ZeroPadding2D(padding=((0,1),(0,1)))(d_x)\n",
        "\t\td_x = tf.keras.layers.LeakyReLU(alpha=0.2)(d_x)\n",
        "\t\td_x = tf.keras.layers.Dropout(0.25)(d_x)\n",
        "\t\td_x = tf.keras.layers.BatchNormalization(momentum=0.8)(d_x)\n",
        "\t\td_x = tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, padding=\"same\")(d_x)\n",
        "\t\td_x = tf.keras.layers.LeakyReLU(alpha=0.2)(d_x)\n",
        "\t\td_x = tf.keras.layers.Dropout(0.25)(d_x)\n",
        "\t\td_x = tf.keras.layers.BatchNormalization(momentum=0.8)(d_x)\n",
        "\t\td_x = tf.keras.layers.Conv2D(128, kernel_size=3, strides=1, padding=\"same\")(d_x)\n",
        "\t\td_x = tf.keras.layers.LeakyReLU(alpha=0.2)(d_x)\n",
        "\t\td_x = tf.keras.layers.Dropout(0.25)(d_x)\n",
        "\t\td_x = tf.keras.layers.Flatten()(d_x)\n",
        "\t\td_x = tf.keras.layers.Dense(self.latent_dim, activation=\"relu\")(d_x)\n",
        "\t\td_x2 = tf.keras.layers.Dense(self.latent_dim*3, activation=\"relu\")(d_x)\n",
        "\t\tvalidity = tf.keras.layers.Dense(self.num_classes, activation=\"softmax\")(d_x2)\n",
        "\t\t#d_x = tf.keras.layers.Dense(1, activation=\"sigmoid\")(d_x)\n",
        "\n",
        "\t\treturn tf.keras.Model(inputs=img_input,outputs=[validity,d_x])\n",
        "\n",
        "\t\n",
        "\tdef train(self, x_train, y_train, epochs, batch_size=128, sample_interval=50):\n",
        " \n",
        "\t\t# Load the dataset\n",
        "\t\t#(X_train, y_train), (_, _) = mnist.load_data()\n",
        "\t\t#X_train, y_train = self.X_train, self.y_train\n",
        " \n",
        "\t\t# Configure inputs\n",
        "\t\t#X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "\t\tx_train = np.expand_dims(x_train, axis=3)\n",
        "\t\ty_train = y_train.reshape(-1, 1)\n",
        " \n",
        "\t\t# Adversarial ground truths\n",
        "\t\tvalid = np.ones((batch_size, 1))\n",
        "\t\tfake = np.zeros((batch_size, 1))\n",
        " \n",
        "\t\t# Loss output\n",
        "\t\tg_loss_epochs = np.zeros((epochs, 1))\n",
        "\t\td_loss_epochs = np.zeros((epochs, 1))\n",
        "\t\te_loss_epochs = np.zeros((epochs, 1))\n",
        " \n",
        "\t\tfor epoch in range(epochs):\n",
        " \n",
        "\t\t\t# ---------------------\n",
        "\t\t\t#  Train Discriminator\n",
        "\t\t\t# ---------------------\n",
        " \n",
        "\t\t\t# Select a random batch of images\n",
        "\t\t\tidx = np.random.randint(0, x_train.shape[0], batch_size)\n",
        "\t\t\timgs = x_train[idx]\n",
        " \n",
        "\t\t\t# Sample noise as generator input\n",
        "\t\t\tnoise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        " \n",
        "\t\t\t# The labels of the digits that the generator tries to create an\n",
        "\t\t\t# image representation of\n",
        "\t\t\tae_labels = self.AEencoder.predict(imgs)\n",
        "\t\t\t#print(sampled_labels[1])\n",
        "\t\t\t#print(type(sampled_labels))\n",
        "\t\t\t# Generate a half batch of new images\n",
        "\t\t\tgen_imgs = self.generator.predict([noise, ae_labels])\n",
        "\t\t\t#label_10_max = np.argmax(label_10,axis=1)\n",
        "\t\t\t#target_labels_max = np.argmax(target_labels,axis=1)\n",
        "\t\t\t#print(target_label)\n",
        "\n",
        "\t\t\t# Image labels. 0-9 if image is valid or 10 if it is generated (fake)\n",
        "\t\t\timg_labels = y_train[idx]\n",
        "\t\t\tfake_labels = 10 * np.ones(img_labels.shape)\n",
        " \n",
        "\t\t\t# Train the discriminator\n",
        "\t\t\td_loss_real = self.discriminator.train_on_batch([imgs, ae_labels], valid)\n",
        "\t\t\td_loss_fake = self.discriminator.train_on_batch([gen_imgs, ae_labels], fake)\n",
        "\t\t\td_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\t\t\t#d_loss = 0.5 * (d_loss_real+d_loss_fake)\n",
        "\n",
        "\t\t\t\n",
        "\t\t\t# Train the generator\n",
        "\t\t\tg_loss = self.combined.train_on_batch([noise, imgs], valid)\n",
        "\n",
        "\t\t\t## Train AEencoder\n",
        "\t\t\timg_de = self.AEdecoder.predict(ae_labels)\n",
        "\t\t\tae_loss = self.combined_AE.train_on_batch(imgs,img_de)\n",
        "\t\t\t\n",
        "\t\t\tg_loss_epochs[epoch] = g_loss\n",
        "\t\t\td_loss_epochs[epoch] = d_loss[0]\n",
        "\t\t\te_loss_epochs[epoch] = ae_loss\n",
        "\n",
        "\n",
        "\t\t\t# If at save interval => save generated image samples\n",
        "\t\t\tif epoch % sample_interval == 0:\n",
        "\t\t\t\t# Plot the progress\n",
        "\t\t\t\tprint (\"Epoch: %d [D loss: %f] [G loss: %f] [AE loss: %f] \" % (epoch, d_loss[0],  g_loss,ae_loss))\n",
        "\t\t\t\t#do not save model\n",
        "\t\t\t\t#self.save_model()\n",
        "\t\t\t\tself.sample_images(epoch,ae_labels,smp_rows=2, smp_cols=10, save_img=False)\n",
        " \n",
        "\t\treturn g_loss_epochs, d_loss_epochs,e_loss_epochs\n",
        " \n",
        " \n",
        "\t#row, cols to be sampled\n",
        "\tdef sample_images(self,epoch,sampled_labels, smp_rows=5, smp_cols=10, save_img=True, fig_size=(8, 3)):\n",
        "\t\tr, c = smp_rows, smp_cols\n",
        "\t\tnoise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "\t\tidx = np.random.randint(0, sampled_labels.shape[0], r*c)\n",
        "\t\tlabels = sampled_labels[idx]\n",
        "\t\t#sampled_labels = self.encoder.predict(img)\n",
        "\t\t#sampled_labels = sampled_labels.reshape(100)\n",
        "\t\tgen_imgs = self.generator.predict([noise,labels.reshape(r*c,self.encoder_dim)])\n",
        "\t\t# Rescale images 0 - 1\n",
        "\t\tgen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "\t\t#plt.figure(figsize=fig_size)\n",
        "\t\tfig, axs = plt.subplots(r, c, figsize=fig_size)\n",
        "\t\tcnt = 0\n",
        "\t\tfor i in range(r):\n",
        "\t\t\tfor j in range(c):\n",
        "\t\t\t\t\n",
        "\t\t\t\taxs[i,j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
        "\t\t\t\taxs[i,j].axis('off')\n",
        "\t\t\t\tcnt += 1\n",
        "\t\tif save_img:\n",
        "\t\t\tfig.savefig(\"%d.png\" % epoch)\n",
        "\t\telse:\n",
        "\t\t\t#plt.figure(figsize=fig_size)\n",
        "\t\t\tplt.show()\n",
        "\t\tplt.close()\n",
        " \n",
        "\tdef sample_single_image(self, noise, label):\n",
        "\t\tgen_imgs = self.generator.predict([noise, np.array(label).reshape((1, ))])\n",
        "\t\t# Rescale images 0 - 1\n",
        "\t\tgen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\t\tplt.imshow(gen_imgs[0, :, :, 0], cmap='gray')\n",
        " \n",
        "\tdef save_model(self):\n",
        " \n",
        "\t\tdef save(model, model_name):\n",
        "\t\t\tmodel_path = \"/content/drive/My Drive/%s.json\" % model_name\n",
        "\t\t\tweights_path = \"/content/drive/My Drive/%s_weights.hdf5\" % model_name\n",
        "\t\t\toptions = {\"file_arch\": model_path,\n",
        "\t\t\t\t\t\t\"file_weight\": weights_path}\n",
        "\t\t\tjson_string = model.to_json()\n",
        "\t\t\t#\n",
        "\t\t\topen(options['file_arch'], 'w').write(json_string)\n",
        "\t\t\tmodel.save_weights(options['file_weight'])\n",
        " \n",
        "\t\tsave(self.generator, \"generator_AE_mnist_v1_encoder_dim-100\")\n",
        "\t\tsave(self.discriminator, \"discriminator_AE_mnist_v1_encoder_dim-100\")\n",
        "\t\tsave(self.encoder, \"encoder_AE_mnist_v1_encoder_dim-100\")\n",
        "\t\tsave(self.combined, \"combined_AE_mnist_v1_encoder_dim-100\")\n",
        "\n",
        "fahsion_acgan = ACGAN(28, 28, 1, 10)\n",
        "g_loss, d_loss,e_loss = fahsion_acgan.train(x_train,y_train, epochs=200000, batch_size=200,sample_interval=2000)\n",
        "fahsion_acgan.save_model()\n",
        "def plot_gan_losses(g_loss, d_loss,e_loss):\n",
        "\tplt.plot(g_loss)\n",
        "\tplt.plot(d_loss)\n",
        "\tplt.plot(e_loss)\n",
        "\tplt.title('GAN Loss Evaluation')\n",
        "\tplt.ylabel('')\n",
        "\tplt.xlabel('epoch')\n",
        "\tplt.legend(['Generator', 'Discriminator','Encoder'],loc='upper right')\n",
        "\tplt.show()\n",
        " \n",
        "plt.style.use('seaborn-white')\n",
        "plot_gan_losses(g_loss, d_loss,e_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28)\n",
            "y_train.shape:(60000,)\n",
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_14 (InputLayer)        [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 1152)              116352    \n",
            "_________________________________________________________________\n",
            "reshape_7 (Reshape)          (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTr (None, 7, 7, 32)          36896     \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_5 (Conv2DTr (None, 14, 14, 128)       36992     \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_6 (Conv2DTr (None, 28, 28, 64)        73792     \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTr (None, 28, 28, 1)         577       \n",
            "_________________________________________________________________\n",
            "decoder_output (Activation)  (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 264,609\n",
            "Trainable params: 264,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}